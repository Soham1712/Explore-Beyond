{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TE3l0TKf8kD3"
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict(X_test)\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "https://www.kaggle.com/code/nkitgupta/evaluation-metrics-for-multi-class-classification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Epp3gJIb5B8O"
      },
      "source": [
        "ROC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7xXUttzy5AwV"
      },
      "outputs": [],
      "source": [
        "# Plots the Probability Distributions and the ROC Curves One vs Rest\n",
        "plt.figure(figsize = (12, 8))\n",
        "bins = [i/20 for i in range(20)] + [1]\n",
        "\n",
        "## If the line below gives error manually write classes into the variable\n",
        "classes = model.classes_\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "roc_auc_ovr = {}\n",
        "for i in range(len(classes)):\n",
        "    # Gets the class\n",
        "    c = classes[i]\n",
        "    \n",
        "    # Prepares an auxiliar dataframe to help with the plots\n",
        "    df_aux = X_test.copy()\n",
        "    df_aux['class'] = [1 if y == c else 0 for y in y_test]\n",
        "    df_aux['prob'] = y_proba[:, i]\n",
        "    df_aux = df_aux.reset_index(drop = True)\n",
        "    \n",
        "    # Plots the probability distribution for the class and the rest\n",
        "    ax = plt.subplot(2, 3, i+1)\n",
        "    sns.histplot(x = \"prob\", data = df_aux, hue = 'class', color = 'b', ax = ax, bins = bins)\n",
        "    ax.set_title(c)\n",
        "    ax.legend([f\"Class: {c}\", \"Rest\"])\n",
        "    ax.set_xlabel(f\"P(x = {c})\")\n",
        "    \n",
        "    # Calculates the ROC Coordinates and plots the ROC Curves\n",
        "    ax_bottom = plt.subplot(2, 3, i+4)\n",
        "    tpr, fpr = get_all_roc_coordinates(df_aux['class'], df_aux['prob'])\n",
        "    plot_roc_curve(tpr, fpr, scatter = False, ax = ax_bottom)\n",
        "    ax_bottom.set_title(\"ROC Curve OvR\")\n",
        "    \n",
        "    # Calculates the ROC AUC OvR\n",
        "    roc_auc_ovr[c] = roc_auc_score(df_aux['class'], df_aux['prob'])\n",
        "plt.tight_layout()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVSV0mdT5G8n"
      },
      "source": [
        "F1 Score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjLs9gQS5YQC"
      },
      "outputs": [],
      "source": [
        "def macro_f1(y_true, y_pred):\n",
        "\n",
        "    # find the number of classes\n",
        "    num_classes = len(np.unique(y_true))\n",
        "\n",
        "    # initialize f1 to 0\n",
        "    f1 = 0\n",
        "    \n",
        "    # loop over all classes\n",
        "    for class_ in list(y_true.unique()):\n",
        "        \n",
        "        # all classes except current are considered negative\n",
        "        temp_true = [1 if p == class_ else 0 for p in y_true]\n",
        "        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n",
        "        \n",
        "        \n",
        "        # compute true positive for current class\n",
        "        tp = true_positive(temp_true, temp_pred)\n",
        "        \n",
        "        # compute false negative for current class\n",
        "        fn = false_negative(temp_true, temp_pred)\n",
        "        \n",
        "        # compute false positive for current class\n",
        "        fp = false_positive(temp_true, temp_pred)\n",
        "        \n",
        "        \n",
        "        # compute recall for current class\n",
        "        temp_recall = tp / (tp + fn + 1e-6)\n",
        "        \n",
        "        # compute precision for current class\n",
        "        temp_precision = tp / (tp + fp + 1e-6)\n",
        "        \n",
        "        \n",
        "        temp_f1 = 2 * temp_precision * temp_recall / (temp_precision + temp_recall + 1e-6)\n",
        "        \n",
        "        # keep adding f1 score for all classes\n",
        "        f1 += temp_f1\n",
        "        \n",
        "    # calculate and return average f1 score over all classes\n",
        "    f1 /= num_classes\n",
        "    \n",
        "    return f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kP-mIQWYVU5X"
      },
      "outputs": [],
      "source": [
        "print(f\"Macro-averaged f1 score : {macro_f1(y_test, y_pred)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZyWwq9iV1yh"
      },
      "outputs": [],
      "source": [
        "def micro_f1(y_true, y_pred):\n",
        "\n",
        "\n",
        "    #micro-averaged precision score\n",
        "    P = micro_precision(y_true, y_pred)\n",
        "\n",
        "    #micro-averaged recall score\n",
        "    R = micro_recall(y_true, y_pred)\n",
        "\n",
        "    #micro averaged f1 score\n",
        "    f1 = 2*P*R / (P + R)    \n",
        "\n",
        "    return f1\n",
        "\n",
        "print(f\"Micro-averaged recall score : {micro_f1(y_test, y_pred)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqnNtZ9257lr"
      },
      "source": [
        "Precision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXaI8J3k6m1H"
      },
      "outputs": [],
      "source": [
        "import pandas\n",
        "import argparse\n",
        "import numpy\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "\n",
        "#there are 3 Types of precision in case of Multi-class classification. \n",
        "#1. Macro averaged precision\n",
        "#2. Micro averaged precision\n",
        "#3. Weighted precision\n",
        "\n",
        "def true_positive(y_true, y_pred):\n",
        "    tp = 0\n",
        "    for yt, yp in zip(y_true, y_pred):\n",
        "        if yt == 1 and yp == 1:\n",
        "            tp += 1\n",
        "    return tp\n",
        "    \n",
        "def true_negative(y_true, y_pred):\n",
        "    tn = 0\n",
        "    for yt, yp in zip(y_true, y_pred):\n",
        "        if yt == 0 and yp == 0:\n",
        "            tn += 1\n",
        "    return tn\n",
        "    \n",
        "def false_positive(y_true, y_pred):\n",
        "    fp = 0\n",
        "    for yt, yp in zip(y_true, y_pred):\n",
        "        if yt == 0 and yp == 1:\n",
        "            fp += 1\n",
        "    return fp\n",
        "    \n",
        "def false_negative(y_true, y_pred):\n",
        "    fn = 0\n",
        "    for yt, yp in zip(y_true, y_pred):\n",
        "        if yt == 1 and yp == 0:\n",
        "            fn += 1\n",
        "    return fn\n",
        "\n",
        "def precision(y_test, y_pred):\n",
        "    tp =true_positive(y_test, y_pred)\n",
        "    fp = false_positive(y_test, y_pred)\n",
        "    try:\n",
        "        return(tp/(tp+fp))\n",
        "    except ZeroDivisionError:\n",
        "        return 0\n",
        "\n",
        "def Macro_averaged_precision(y_test, predictions):\n",
        "    precisions = []\n",
        "    for i in range(1,5):\n",
        "        temp_ytest = [1 if x == i else 0 for x in y_test]\n",
        "        temp_ypred = [1 if x == i else 0 for x in predictions]\n",
        "        print(temp_ypred)\n",
        "        print(temp_ytest)\n",
        "        prec = precision(temp_ytest, temp_ypred)\n",
        "        precisions.append(prec)\n",
        "    \n",
        "    return (sum(precisions)/len(precisions))\n",
        "         \n",
        "def Micro_averaged_precision(y_test, predictions):\n",
        "    tp = 0\n",
        "    fp = 0\n",
        "    for i in range(1,5):\n",
        "        temp_ytest = [1 if x == i else 0 for x in y_test]\n",
        "        temp_ypred = [1 if x == i else 0 for x in predictions]\n",
        "\n",
        "        tp += true_positive(temp_ytest, temp_ypred)\n",
        "        fp += false_positive(temp_ytest, temp_ypred)\n",
        "\n",
        "    precisions = tp / (tp + fp)\n",
        "\n",
        "    return precisions\n",
        "\n",
        "def weighted_precision(y_test, predictions):\n",
        "    num_classes = len(numpy.unique(y_test))\n",
        "    #coutns for every class\n",
        "    precision = 0\n",
        "    for i in range(1, num_classes):\n",
        "        temp_ytest = [1 if x == i else 0 for x in y_test]\n",
        "        temp_ypred = [1 if x == i else 0 for x in predictions]\n",
        "\n",
        "        tp = true_positive(temp_ytest, temp_ypred)\n",
        "        fp = false_positive(temp_ytest, temp_ypred)\n",
        "        \n",
        "        try:\n",
        "            preai = tp / (tp+fp)\n",
        "        except ZeroDivisionError:\n",
        "            preai = 0\n",
        "\n",
        "        weighted = preai*sum(temp_ytest)\n",
        "\n",
        "        precision += weighted\n",
        "\n",
        "    precision = precision/len(y_test)\n",
        "    return precision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjSDGjobUeXA"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(X_test)\n",
        "\n",
        "print(\"Macro precision is:\", Macro_averaged_precision(y_test, predictions))\n",
        "print(\"Micro precision is:\", Micro_averaged_precision(y_test, predictions))\n",
        "print(\"Weighted precision is:\", weighted_precision(y_test, predictions))\n",
        "print(\"sklearn Macro\", metrics.precision_score(y_test, predictions, average = \"macro\"))\n",
        "print(\"sklearn Micro\", metrics.precision_score(y_test, predictions, average = \"micro\"))\n",
        "print(\"sklearn weighted\", metrics.precision_score(y_test, predictions, average = \"weighted\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ii2ZuKzT6rn0"
      },
      "source": [
        "Recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSayxM8a6sYl"
      },
      "outputs": [],
      "source": [
        "import pandas\n",
        "import argparse\n",
        "import numpy\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "\n",
        "#there are 3 Types of recall in case of Multi-class classification. \n",
        "#1. Macro averaged recall\n",
        "#2. Micro averaged recall\n",
        "#3. Weighted recall\n",
        "\n",
        "def true_positive(y_true, y_pred):\n",
        "    tp = 0\n",
        "    for yt, yp in zip(y_true, y_pred):\n",
        "        if yt == 1 and yp == 1:\n",
        "            tp += 1\n",
        "    return tp\n",
        "    \n",
        "def true_negative(y_true, y_pred):\n",
        "    tn = 0\n",
        "    for yt, yp in zip(y_true, y_pred):\n",
        "        if yt == 0 and yp == 0:\n",
        "            tn += 1\n",
        "    return tn\n",
        "    \n",
        "def false_positive(y_true, y_pred):\n",
        "    fp = 0\n",
        "    for yt, yp in zip(y_true, y_pred):\n",
        "        if yt == 0 and yp == 1:\n",
        "            fp += 1\n",
        "    return fp\n",
        "    \n",
        "def false_negative(y_true, y_pred):\n",
        "    fn = 0\n",
        "    for yt, yp in zip(y_true, y_pred):\n",
        "        if yt == 1 and yp == 0:\n",
        "            fn += 1\n",
        "    return fn\n",
        "\n",
        "def recall(y_test, y_pred):\n",
        "    tp = true_positive(y_test, y_pred)\n",
        "    fn = false_negative(y_test, y_pred)\n",
        "    return(tp/(tp+fn))\n",
        "\n",
        "def Macro_averaged_recall(y_test, predictions):\n",
        "    recalls = []\n",
        "    for i in range(1,5):\n",
        "        temp_ytest = [1 if x == i else 0 for x in y_test]\n",
        "        temp_ypred = [1 if x == i else 0 for x in predictions]\n",
        "        print(temp_ypred)\n",
        "        print(temp_ytest)\n",
        "        rec = recall(temp_ytest, temp_ypred)\n",
        "        recalls.append(rec)\n",
        "    \n",
        "    return (sum(recalls)/len(recalls))\n",
        "         \n",
        "def Micro_averaged_recall(y_test, predictions):\n",
        "    tp = 0\n",
        "    tn = 0\n",
        "    for i in range(1,5):\n",
        "        temp_ytest = [1 if x == i else 0 for x in y_test]\n",
        "        temp_ypred = [1 if x == i else 0 for x in predictions]\n",
        "\n",
        "        tp += true_positive(temp_ytest, temp_ypred)\n",
        "        tn += true_negative(temp_ytest, temp_ypred)\n",
        "\n",
        "    recall = tp / (tp + tn)\n",
        "\n",
        "    return recall\n",
        "\n",
        "def weighted_recall(y_test, predictions):\n",
        "    num_classes = len(numpy.unique(y_test))\n",
        "    #counts for every class\n",
        "    recall = 0\n",
        "    for i in range(1, num_classes):\n",
        "        temp_ytest = [1 if x == i else 0 for x in y_test]\n",
        "        temp_ypred = [1 if x == i else 0 for x in predictions]\n",
        "\n",
        "        tp = true_positive(temp_ytest, temp_ypred)\n",
        "        tn = true_negative(temp_ytest, temp_ypred)\n",
        "        \n",
        "        try:\n",
        "            rec = tp / (tp+tn)\n",
        "        except ZeroDivisionError:\n",
        "            rec = 0\n",
        "\n",
        "        weighted = rec*sum(temp_ytest)\n",
        "\n",
        "        recall += weighted\n",
        "\n",
        "    recall = recall/len(y_test)\n",
        "    return recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tn9qyOu1VCeo"
      },
      "outputs": [],
      "source": [
        "print(\"Macro recall is:\", Macro_averaged_recall(y_test, predictions))\n",
        "print(\"Micro recall is:\", Micro_averaged_recall(y_test, predictions))\n",
        "print(\"Weighted recall is:\", weighted_recall(y_test, predictions))\n",
        "print(\"sklearn Macro\", metrics.recall_score(y_test, predictions, average = \"macro\"))\n",
        "print(\"sklearn Micro\", metrics.recall_score(y_test, predictions, average = \"micro\"))\n",
        "print(\"sklearn weighted\", metrics.recall_score(y_test, predictions, average = \"weighted\"))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "machine_learning",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13 (default, Oct 19 2022, 10:19:43) [MSC v.1916 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "72fd977071d8a325c89946734ebdd84ca04cd1ddcd4553ef7fac0759bd651065"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
